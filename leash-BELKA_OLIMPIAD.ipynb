{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "0H7m5buU-e-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs the RDKit library, which is used for cheminformatics (processing chemical information). The RDKit library is necessary for working with molecular data."
      ],
      "metadata": {
        "id": "Pfz-jSFr-m8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSK2oZOAHEMD",
        "outputId": "1b83c4de-b7fa-4cfe-8f53-01db32b1a3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2024.3.3)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE49cD5pE7gK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, Normalizer, MinMaxScaler, QuantileTransformer, OneHotEncoder\n",
        "from sklearn.metrics import average_precision_score, roc_curve, accuracy_score, roc_auc_score, classification_report, precision_recall_curve, auc, f1_score, precision_score, recall_score, fbeta_score, confusion_matrix\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts a molecule in SMILES format to a fingerprint using the Morgan (circular) fingerprint method. This transformation is often used to convert molecular structures into numerical representations suitable for machine learning models."
      ],
      "metadata": {
        "id": "SDoi4DDg-4EP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo43kLAiRnR_"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
        "\n",
        "def smiles_to_fingerprint(mol, radius=2, bits=256):\n",
        "    generator = GetMorganGenerator(radius=radius, fpSize=bits)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return list(generator.GetFingerprint(mol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS_8ey7zCygH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function tests different scalers on the training and testing data using a specified model (here, RandomForestClassifier). It returns the F1 score for each scaler, allowing comparison of their performance."
      ],
      "metadata": {
        "id": "prFZhE2a_YyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oneQKaSvFBqT"
      },
      "outputs": [],
      "source": [
        "def test_scaler(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.DataFrame, y_test: pd.DataFrame, model)-> dict:\n",
        "  scaler_list = [RobustScaler(), StandardScaler(), Normalizer(), MinMaxScaler(), QuantileTransformer(output_distribution='uniform')]\n",
        "  my_dict = {}\n",
        "  for scaler in scaler_list:\n",
        "      pipeline = make_pipeline(\n",
        "          scaler,\n",
        "          model\n",
        "      )\n",
        "      pipeline.fit(X_train, y_train)\n",
        "      y_pred = pipeline.predict(X_test)\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      my_dict[f'{scaler.__class__.__name__}'] = accuracy\n",
        "  # print(f'Best scaler: {min(my_dict, key=my_dict.get)}')\n",
        "  return {min(my_dict, key=my_dict.get) : my_dict[f'{min(my_dict, key=my_dict.get)}']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9XBtQXpFFLD"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/prepared_train.csv'\n",
        "# Загрузите CSV-файл в pandas DataFrame\n",
        "train = pd.read_csv(train_path)\n",
        "\n",
        "# Выполните фильтрацию и объединение данных\n",
        "train = pd.concat([\n",
        "    train[train['binds'] == 0].sample(n=30000, random_state=42),\n",
        "    train[train['binds'] == 1].sample(n=30000, random_state=42)\n",
        "]).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upFTrh8Z6-WN"
      },
      "outputs": [],
      "source": [
        "df = train.copy()\n",
        "\n",
        "df['mol'] = df['molecule_smiles'].apply(Chem.MolFromSmiles)\n",
        "df['bb1_mol'] = df['buildingblock1_smiles'].apply(Chem.MolFromSmiles)\n",
        "df['bb2_mol'] = df['buildingblock2_smiles'].apply(Chem.MolFromSmiles)\n",
        "df['bb3_mol'] = df['buildingblock3_smiles'].apply(Chem.MolFromSmiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qc0K5jkEQ2t"
      },
      "outputs": [],
      "source": [
        "df['ecfp'] = df['mol'].apply(smiles_to_fingerprint) #creating finger print\n",
        "df['molecular_weight'] = df['mol'].apply(Descriptors.MolWt)\n",
        "df['bb1_w'] = df['bb1_mol'].apply(Descriptors.MolWt)\n",
        "df['bb2_w'] = df['bb2_mol'].apply(Descriptors.MolWt)\n",
        "df['bb3_w'] = df['bb3_mol'].apply(Descriptors.MolWt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvuXeIcIVQeL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmMCXZGBK49x"
      },
      "outputs": [],
      "source": [
        "df = pd.get_dummies(df, columns=['protein_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brGGZeEFUp6J"
      },
      "outputs": [],
      "source": [
        "# Преобразование логических значений в целые числа\n",
        "df['protein_name_BRD4'] = df['protein_name_BRD4'] /1\n",
        "df['protein_name_HSA'] = df['protein_name_HSA'] /1\n",
        "df['protein_name_sEH'] = df['protein_name_sEH'] /1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgm_eVztUOiy"
      },
      "outputs": [],
      "source": [
        "# Объединение всех колонок в один большой список\n",
        "df['combined'] = df.apply(lambda row: row['ecfp'] + [row['molecular_weight'], row['bb1_w'], row['bb2_w'], row['bb3_w'], row['protein_name_BRD4'], row['protein_name_HSA'], row['protein_name_sEH']], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell separates features (X) and the target variable (y), and splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "ISc4ztCn_JDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n38SnL1RLItK"
      },
      "outputs": [],
      "source": [
        "X = df['combined'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cZiXlVe8V4o"
      },
      "outputs": [],
      "source": [
        "y = df['binds'].tolist()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw5r7I5DYVd3",
        "outputId": "95d7dc46-6c71-4789-caf5-b05acad60eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8891666666666667\n"
          ]
        }
      ],
      "source": [
        "# Обучение модели\n",
        "model = make_pipeline(\n",
        "    RandomForestClassifier(n_estimators=1000, random_state=42, criterion='entropy')\n",
        "    )\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Оценка модели\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FVN3gGpb5rx",
        "outputId": "8b29267e-b6f8-4353-b54a-90c6dfc17d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Average Precision (mAP): 0.96\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "# Calculate the mean average precision\n",
        "map_score = average_precision_score(y_test, y_pred_proba)\n",
        "print(f\"Mean Average Precision (mAP): {map_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/\\\n",
        "\n",
        "Mean Average Precision (mAP) is a more nuanced metric often used in information retrieval and classification tasks, especially when dealing with imbalanced classes. It is the average of the precision values at different recall levels.\n",
        "\n",
        "Precision measures how many of the items retrieved by your model are relevant.\n",
        "Recall measures how many of the relevant items are retrieved by your model.\n",
        "mAP essentially combines these two metrics to give you a single value that represents the average precision across all recall levels for your model.\n",
        "\n",
        "A mAP of 0.96 means that on average, your model's precision is very high across different levels of recall. This is a strong indication that your model is not only good at retrieving relevant items but also maintains high precision when doing so.\n",
        "\n",
        "Key Points:\n",
        "Accuracy (0.8892): Indicates that your model's overall correctness is about 88.92%. This means it gets about 88.92% of its predictions right.\n",
        "mAP (0.96): Shows that your model's precision is consistently high across different levels of recall, with an average precision score of 0.96. This suggests your model is very good at retrieving relevant items and has a high level of reliability in its predictions.\n",
        "These metrics suggest that your model is performing well, both in general accuracy and in maintaining high precision across various levels of recall."
      ],
      "metadata": {
        "id": "guggmyF3A8RP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxjha1ub5ot",
        "outputId": "e5b61b8d-6398-4263-ac17-8b7c9756d896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision per class: [0.87016977 0.90937231]\n",
            "Recall per class: [0.91081401 0.86816615]\n",
            "F1-score per class: [0.89002811 0.88829162]\n",
            "\n",
            "Micro-average Precision: 0.8891666666666667\n",
            "Micro-average Recall: 0.8891666666666667\n",
            "Micro-average F1-score: 0.8891666666666667\n",
            "\n",
            "Macro-average Precision: 0.8897710392727531\n",
            "Macro-average Recall: 0.8894900796485992\n",
            "Macro-average F1-score: 0.8891598653928514\n",
            "\n",
            "Weighted-average Precision: 0.8900683252573686\n",
            "Weighted-average Recall: 0.8891666666666667\n",
            "Weighted-average F1-score: 0.8891466969690816\n"
          ]
        }
      ],
      "source": [
        "# Вычисление precision, recall и F1-score для каждого класса\n",
        "precision_per_class = precision_score(y_test, y_pred, average=None)\n",
        "recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "f1_score_per_class = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "# Вычисление микроусредненных метрик\n",
        "micro_precision = precision_score(y_test, y_pred, average='micro')\n",
        "micro_recall = recall_score(y_test, y_pred, average='micro')\n",
        "micro_f1_score = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "# Вычисление макроусредненных метрик\n",
        "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
        "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
        "macro_f1_score = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Вычисление взвешенных метрик\n",
        "weighted_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "weighted_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "weighted_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Precision per class:\", precision_per_class)\n",
        "print(\"Recall per class:\", recall_per_class)\n",
        "print(\"F1-score per class:\", f1_score_per_class)\n",
        "print(\"\\nMicro-average Precision:\", micro_precision)\n",
        "print(\"Micro-average Recall:\", micro_recall)\n",
        "print(\"Micro-average F1-score:\", micro_f1_score)\n",
        "print(\"\\nMacro-average Precision:\", macro_precision)\n",
        "print(\"Macro-average Recall:\", macro_recall)\n",
        "print(\"Macro-average F1-score:\", macro_f1_score)\n",
        "print(\"\\nWeighted-average Precision:\", weighted_precision)\n",
        "print(\"Weighted-average Recall:\", weighted_recall)\n",
        "print(\"Weighted-average F1-score:\", weighted_f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's break down these metrics in detail:\n",
        "\n",
        "### Precision, Recall, and F1-score per Class\n",
        "- **Precision per class:**\n",
        "  - Class 0: 0.87016977\n",
        "  - Class 1: 0.90937231\n",
        "  - Precision measures the proportion of true positive predictions out of all positive predictions. For class 0, 87.02% of the predicted positives are true positives. For class 1, 90.94% of the predicted positives are true positives.\n",
        "\n",
        "- **Recall per class:**\n",
        "  - Class 0: 0.91081401\n",
        "  - Class 1: 0.86816615\n",
        "  - Recall measures the proportion of true positive predictions out of all actual positives. For class 0, 91.08% of the actual positives are correctly identified. For class 1, 86.82% of the actual positives are correctly identified.\n",
        "\n",
        "- **F1-score per class:**\n",
        "  - Class 0: 0.89002811\n",
        "  - Class 1: 0.88829162\n",
        "  - The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both. For class 0, the F1-score is 0.8900, and for class 1, it is 0.8883.\n",
        "\n",
        "### Micro-Average Metrics\n",
        "- **Micro-average Precision:** 0.8891666666666667\n",
        "- **Micro-average Recall:** 0.8891666666666667\n",
        "- **Micro-average F1-score:** 0.8891666666666667\n",
        "\n",
        "Micro-averaging calculates the metrics globally by counting the total true positives, false negatives, and false positives. It is useful when you want to know the overall performance of your model without focusing on individual classes. Here, all three metrics are 0.8892, indicating consistent performance across all instances.\n",
        "\n",
        "### Macro-Average Metrics\n",
        "- **Macro-average Precision:** 0.8897710392727531\n",
        "- **Macro-average Recall:** 0.8894900796485992\n",
        "- **Macro-average F1-score:** 0.8891598653928514\n",
        "\n",
        "Macro-averaging calculates the metrics for each class independently and then takes the average, giving equal weight to each class regardless of its frequency. This is useful for evaluating the performance of your model on each class individually and then averaging it out. The macro-average precision, recall, and F1-score are all around 0.8892, indicating that the model performs fairly consistently across different classes.\n",
        "\n",
        "### Weighted-Average Metrics\n",
        "- **Weighted-average Precision:** 0.8900683252573686\n",
        "- **Weighted-average Recall:** 0.8891666666666667\n",
        "- **Weighted-average F1-score:** 0.8891466969690816\n",
        "\n",
        "Weighted-averaging calculates the metrics for each class and then takes the average, weighted by the number of instances of each class. This means that classes with more instances will have a bigger impact on the overall metric. The weighted-average precision, recall, and F1-score are very close to the micro-average scores, suggesting that the class distribution does not heavily skew the overall performance metrics.\n",
        "\n",
        "### Summary\n",
        "- **Per Class Metrics:** Shows precision, recall, and F1-score for each class individually.\n",
        "- **Micro-Average:** Evaluates overall performance considering all instances equally.\n",
        "- **Macro-Average:** Evaluates performance by averaging metrics for each class, treating each class equally.\n",
        "- **Weighted-Average:** Evaluates performance by averaging metrics for each class, considering class frequency.\n",
        "\n",
        "These metrics provide a comprehensive view of your model's performance, indicating that it performs consistently well across different classes and on the dataset as a whole."
      ],
      "metadata": {
        "id": "irfDjcgtB2vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing different scalers and methods"
      ],
      "metadata": {
        "id": "nWYLIBEkB6PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop tests different criteria for the LogisticRegression using the previously defined test_scaler function, comparing the performance with different splitting criteria (L1, L2).\n",
        "\n"
      ],
      "metadata": {
        "id": "vMIgnJAW_zc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulrXtC2fLvx7",
        "outputId": "b9e4a84c-df63-41f7-dd3a-6ca964da9768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "l1: {'Normalizer': 0.695}\n",
            "l2: {'Normalizer': 0.5554166666666667}\n"
          ]
        }
      ],
      "source": [
        "penalty_list = ['l1', 'l2'] #, 'elasticnet'\n",
        "for penalty in penalty_list:\n",
        "    if penalty == 'elasticnet':\n",
        "        solver = 'saga'\n",
        "        l1_ratio = 0.5  # or any other value between 0 and 1\n",
        "    else:\n",
        "        solver = 'liblinear'\n",
        "        l1_ratio = None\n",
        "    print(f\"{penalty}: {test_scaler(X_train, X_test, y_train, y_test, LogisticRegression(penalty=penalty, max_iter=1500, solver=solver, l1_ratio=l1_ratio, random_state=42))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop tests different criteria for the DecisionTreeClassifier using the previously defined test_scaler function, comparing the performance with different splitting criteria (gini and entropy).\n",
        "\n"
      ],
      "metadata": {
        "id": "JrC1KImm_e_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bSxAIkPU_Cd",
        "outputId": "ac6ae91f-5969-477a-cded-2c2c67ba1c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gini: {'Normalizer': 0.8046666666666666}\n",
            "entropy: {'Normalizer': 0.8156666666666667}\n",
            "log_loss: {'Normalizer': 0.8156666666666667}\n"
          ]
        }
      ],
      "source": [
        "criterion_list = ['gini', 'entropy', 'log_loss']\n",
        "for criterion in criterion_list:\n",
        "    print(f\"{criterion}: {test_scaler(X_train, X_test, y_train, y_test, DecisionTreeClassifier(random_state=42, criterion=criterion))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop tests different criteria for the RandomForestClassifier using the previously defined test_scaler function, comparing the performance with different splitting criteria (gini and entropy)."
      ],
      "metadata": {
        "id": "isViUX1BACrk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbjDH8iOV5f_",
        "outputId": "4e0e2825-1fa2-440f-e6bc-71ab57f4466e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gini: {'Normalizer': 0.8698333333333333}\n",
            "entropy: {'Normalizer': 0.8706666666666667}\n",
            "log_loss: {'Normalizer': 0.8706666666666667}\n"
          ]
        }
      ],
      "source": [
        "criterion_list = ['gini', 'entropy', 'log_loss']\n",
        "for criterion in criterion_list:\n",
        "    print(f\"{criterion}: {test_scaler(X_train, X_test, y_train, y_test, RandomForestClassifier(random_state=42, criterion=criterion, n_estimators=100))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop tests different loss functions for the GradientBoostingClassifier using the test_scaler function, comparing the performance with different loss functions (log_loss and exponential)."
      ],
      "metadata": {
        "id": "UaFpu5Hk_niD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z9zqwhCYNlO",
        "outputId": "d90150d0-a20f-47fa-acc7-2007dbcd11b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "friedman_mse: {'Normalizer': 0.8300833333333333}\n",
            "squared_error: {'Normalizer': 0.8300833333333333}\n"
          ]
        }
      ],
      "source": [
        "criterion_list = ['friedman_mse', 'squared_error']\n",
        "for criterion in criterion_list:\n",
        "    print(f\"{criterion}: {test_scaler(X_train, X_test, y_train, y_test, GradientBoostingClassifier(random_state=42, criterion=criterion))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop tests different loss functions for the GradientBoostingClassifier using the test_scaler function, comparing the performance with different loss functions (log_loss and exponential)."
      ],
      "metadata": {
        "id": "twbEryG9_q9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JnyqKYysYR3I",
        "outputId": "0b566c84-1bbe-4d39-8c63-8ebfa85af5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log_loss: {'Normalizer': 0.8300833333333333}\n",
            "exponential: {'Normalizer': 0.82725}\n"
          ]
        }
      ],
      "source": [
        "loss_list = ['log_loss', 'exponential']\n",
        "for loss in loss_list:\n",
        "    print(f\"{loss}: {test_scaler(X_train, X_test, y_train, y_test, GradientBoostingClassifier(random_state=42, loss=loss))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SkDIJPKfKUt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}